{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_sdwfHwudy"
      },
      "source": [
        "## Phase-3: Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFEz_Q2-w2BK"
      },
      "source": [
        "Step-0a: Data Ingestion and Initial Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex2zGWivwY93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a23138f-bb58-4d19-e7fb-99c155884c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/multimodal_mammography\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Go to your project folder\n",
        "%cd /content/drive/MyDrive/multimodal_mammography\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M5-HwkUw5Cq"
      },
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "\n",
        "def load_module_from_path(name, path):\n",
        "    spec = importlib.util.spec_from_file_location(name, path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocI2-hsKxC8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda0c9bf-53ac-4462-e87a-3dfc1cf1ac58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Detected Google Colab environment.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted.\n",
            "üì¶ Installing required packages...\n",
            "‚úÖ Dependencies installed.\n",
            " Warnings suppressed.\n",
            "üîÅ Seed set to 42\n",
            " Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Load environment setup\n",
        "env = load_module_from_path(\"env\", \"setup/environment.py\")\n",
        "install = load_module_from_path(\"install\", \"setup/install_colab.py\")\n",
        "_ = load_module_from_path(\"imports\", \"setup/imports.py\")  # No functions to call\n",
        "\n",
        "# Run setup\n",
        "install.install_dependencies()\n",
        "env.suppress_warnings()\n",
        "env.set_seed(42)\n",
        "device = env.get_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0psAZT1xJF-"
      },
      "source": [
        "Step-0b: Loading Required csvs' and extracting/exploring images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUBNcL-wxEi_"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load the dynamic module\n",
        "data_loader = load_module_from_path(\"data_loader\", \"data/load_data.py\")\n",
        "\n",
        "# ‚úÖ Correct CSV paths\n",
        "metadata_path    = \"/content/drive/MyDrive/multimodal_mammography/dataset/csv/metadata.csv\"\n",
        "breast_anno_path = \"/content/drive/MyDrive/multimodal_mammography/dataset/csv/breast-level_annotations.csv\"\n",
        "finding_anno_path = \"/content/drive/MyDrive/multimodal_mammography/dataset/csv/finding_annotations.csv\"\n",
        "\n",
        "# ‚úÖ Load and view data\n",
        "metadata_df, breast_df, finding_df = data_loader.load_mammo_data(\n",
        "    metadata_path,\n",
        "    breast_anno_path,\n",
        "    finding_anno_path,\n",
        "    verbose=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "image_df=pd.read_csv(\"/content/drive/MyDrive/multimodal_mammography/dataset/csv/image_df_upsampled_studywise.csv\")"
      ],
      "metadata": {
        "id": "EPooQpAxLC-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L5ZMlAgLc1X",
        "outputId": "09b98bf8-4c87-424f-f915-4acd8a479cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_id', 'study_id', 'filename', 'birads', 'birads_dir', 'density',\n",
            "       'laterality', 'view_position', 'split', 'finding_categories',\n",
            "       'finding_birads_clean', 'xmin', 'ymin', 'xmax', 'ymax', 'has_bbox',\n",
            "       'age', 'birads_binary', 'birads_cleaned', 'birads_study_level',\n",
            "       'finding_mass', 'finding_suspicious_calcification',\n",
            "       'finding_focal_asymmetry', 'finding_asymmetry',\n",
            "       'finding_global_asymmetry', 'finding_architectural_distortion',\n",
            "       'finding_skin_thickening', 'finding_skin_retraction',\n",
            "       'finding_nipple_retraction', 'finding_suspicious_lymph_node',\n",
            "       'finding_no_finding', 'image_path', 'case_category', 'upsampled'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"/content/drive/MyDrive/multimodal_mammography/dataset/zipped_folder/birads_preprocessed_dataset.zip\"\n",
        "\n",
        "# Destination folder to extract files\n",
        "extract_dir = \"/content/birads_preprocessed_dataset\"\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Extraction complete.\")\n",
        "print(\"Extracted to:\", extract_dir)\n"
      ],
      "metadata": {
        "id": "c-U_axQNf52i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c10b148-c348-4252-be30-e91ab4601c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "Extracted to: /content/birads_preprocessed_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List a few extracted files/folders\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    print(\"Root:\", root)\n",
        "    print(\"Subdirs:\", dirs[:5])   # show first 5 dirs\n",
        "    print(\"Files:\", files[:5])   # show first 5 files\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sxFyOlLG5Nw",
        "outputId": "3df27902-7725-4344-c893-83b76b0a751c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root: /content/birads_preprocessed_dataset\n",
            "Subdirs: ['training', 'test']\n",
            "Files: ['image_df_upsampled_preprocessed.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "\n",
        "def find_small_studies(base_dir, min_images=4):\n",
        "    small_studies = defaultdict(list)\n",
        "\n",
        "    for split in [\"training\", \"test\"]:\n",
        "        for case in [\"normal\", \"abnormal\"]:\n",
        "            case_path = os.path.join(base_dir, split, case)\n",
        "            if not os.path.exists(case_path):\n",
        "                continue\n",
        "\n",
        "            for study in os.listdir(case_path):\n",
        "                study_path = os.path.join(case_path, study)\n",
        "                if not os.path.isdir(study_path):\n",
        "                    continue\n",
        "\n",
        "                imgs = [f for f in os.listdir(study_path) if f.endswith(\".png\")]\n",
        "                if len(imgs) < min_images:\n",
        "                    small_studies[(split, case, study)] = imgs\n",
        "\n",
        "    return small_studies\n",
        "\n",
        "small_studies = find_small_studies(base_dir)\n",
        "\n",
        "if small_studies:\n",
        "    print(\"‚ö†Ô∏è Studies with fewer than 4 images:\")\n",
        "    for (split, case, study), imgs in small_studies.items():\n",
        "        print(f\"- {split}/{case}/{study} -> {len(imgs)} images: {imgs}\")\n",
        "else:\n",
        "    print(\"‚úÖ All studies have at least 4 images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QsZ3A4PKxCu",
        "outputId": "2731dd38-af07-4c17-f60e-98ec0c1bb4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All studies have at least 4 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "splits = [\"training\", \"test\"]\n",
        "classes = [\"normal\", \"abnormal\"]\n",
        "\n",
        "# Dictionary to store study -> image count\n",
        "study_image_counts = {}\n",
        "\n",
        "for split in splits:\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        if not os.path.exists(cls_path):\n",
        "            continue\n",
        "        for study in os.listdir(cls_path):\n",
        "            study_path = os.path.join(cls_path, study)\n",
        "            if os.path.isdir(study_path):\n",
        "                images = [f for f in os.listdir(study_path) if f.endswith(\".png\")]\n",
        "                study_image_counts[study] = len(images)\n",
        "\n",
        "# Summarize the distribution of images per study\n",
        "count_distribution = Counter(study_image_counts.values())\n",
        "print(\"Image count per study distribution:\")\n",
        "for n_images, n_studies in sorted(count_distribution.items()):\n",
        "    print(f\"{n_images} images: {n_studies} studies\")\n",
        "\n",
        "# Optional: total studies\n",
        "print(f\"\\nTotal studies counted: {len(study_image_counts)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOxUndswzXO",
        "outputId": "7bced83c-2c43-4367-ec20-6395f224703e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image count per study distribution:\n",
            "4 images: 7999 studies\n",
            "\n",
            "Total studies counted: 7999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # import tqdm\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "original_csv = os.path.join(base_dir, \"image_df_upsampled_preprocessed.csv\")\n",
        "fixed_csv = os.path.join(base_dir, \"image_df_preprocessed_fixed.csv\")\n",
        "\n",
        "# Load original CSV\n",
        "df_orig = pd.read_csv(original_csv)\n",
        "\n",
        "# Ensure string types for safe matching\n",
        "df_orig[\"study_id\"] = df_orig[\"study_id\"].astype(str)\n",
        "df_orig[\"filename\"] = df_orig[\"filename\"].astype(str)\n",
        "\n",
        "# Prepare list for final rows\n",
        "rows = []\n",
        "\n",
        "splits = [\"training\", \"test\"]\n",
        "classes = [\"normal\", \"abnormal\"]\n",
        "\n",
        "for split in splits:\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    if not os.path.exists(split_path):\n",
        "        continue\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        if not os.path.exists(cls_path):\n",
        "            continue\n",
        "\n",
        "        study_list = [s for s in os.listdir(cls_path) if os.path.isdir(os.path.join(cls_path, s))]\n",
        "        for study in tqdm(study_list, desc=f\"{split}/{cls} studies\"):\n",
        "            study_path = os.path.join(cls_path, study)\n",
        "\n",
        "            images = sorted([f for f in os.listdir(study_path) if f.endswith(\".png\")])\n",
        "            if len(images) != 4:\n",
        "                continue  # only keep studies with exactly 4 images\n",
        "\n",
        "            for img in images:\n",
        "                # Try to get metadata from original CSV\n",
        "                match = df_orig[(df_orig[\"study_id\"] == study) &\n",
        "                                (df_orig[\"filename\"] == img)]\n",
        "                if not match.empty:\n",
        "                    row = match.iloc[0].copy()\n",
        "                    row[\"image_path\"] = os.path.join(study_path, img)  # update path\n",
        "                else:\n",
        "                    # If missing in original CSV, create minimal row with placeholders\n",
        "                    row = {col: -1 for col in df_orig.columns}  # -1 as placeholder\n",
        "                    row[\"study_id\"] = study\n",
        "                    row[\"filename\"] = img\n",
        "                    row[\"image_path\"] = os.path.join(study_path, img)\n",
        "                    row[\"split\"] = split\n",
        "                    row[\"case_category\"] = cls\n",
        "\n",
        "                rows.append(row)\n",
        "\n",
        "# Build DataFrame\n",
        "df_fixed = pd.DataFrame(rows)\n",
        "\n",
        "# Save CSV\n",
        "df_fixed.to_csv(fixed_csv, index=False)\n",
        "\n",
        "# Summary\n",
        "print(f\"‚úÖ Fixed CSV saved at: {fixed_csv}\")\n",
        "print(f\"Total studies included: {df_fixed['study_id'].nunique()}\")\n",
        "print(f\"Total images included: {len(df_fixed)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkyZcs3eweu1",
        "outputId": "96c8c0f8-b310-48a7-e5ca-3aca57c87e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training/normal studies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5065/5065 [02:01<00:00, 41.55it/s]\n",
            "training/abnormal studies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1934/1934 [00:44<00:00, 43.48it/s]\n",
            "test/normal studies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 916/916 [00:22<00:00, 40.32it/s]\n",
            "test/abnormal studies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84/84 [00:01<00:00, 47.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed CSV saved at: /content/birads_preprocessed_dataset/image_df_preprocessed_fixed.csv\n",
            "Total studies included: 7999\n",
            "Total images included: 31996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_fixed.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaHhbmSc1Ady",
        "outputId": "279815b4-ec76-48dc-ec19-a4eaafdfd46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_id', 'study_id', 'filename', 'birads', 'birads_dir', 'density',\n",
            "       'laterality', 'view_position', 'split', 'finding_categories',\n",
            "       'finding_birads_clean', 'xmin', 'ymin', 'xmax', 'ymax', 'has_bbox',\n",
            "       'age', 'birads_binary', 'birads_cleaned', 'birads_study_level',\n",
            "       'finding_mass', 'finding_suspicious_calcification',\n",
            "       'finding_focal_asymmetry', 'finding_asymmetry',\n",
            "       'finding_global_asymmetry', 'finding_architectural_distortion',\n",
            "       'finding_skin_thickening', 'finding_skin_retraction',\n",
            "       'finding_nipple_retraction', 'finding_suspicious_lymph_node',\n",
            "       'finding_no_finding', 'image_path', 'case_category', 'upsampled',\n",
            "       'preprocessed_path'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "csv_path = os.path.join(base_dir, \"image_df_preprocessed_fixed.csv\")\n",
        "\n",
        "# Load CSV metadata\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Build sets for quick lookup\n",
        "expected_study_ids = set(df[\"study_id\"].astype(str).unique())\n",
        "expected_image_ids = set(df[\"image_id\"].astype(str).unique())\n",
        "\n",
        "issues = []\n",
        "\n",
        "# Iterate over splits and classes\n",
        "for split in [\"training\", \"test\"]:\n",
        "    for cls in [\"normal\", \"abnormal\"]:\n",
        "        cls_path = os.path.join(base_dir, split, cls)\n",
        "        if not os.path.exists(cls_path):\n",
        "            issues.append(f\"Missing folder: {cls_path}\")\n",
        "            continue\n",
        "\n",
        "        # Iterate over studies\n",
        "        for study in os.listdir(cls_path):\n",
        "            study_path = os.path.join(cls_path, study)\n",
        "            if not os.path.isdir(study_path):\n",
        "                continue\n",
        "\n",
        "            # Validate study ID\n",
        "            if study not in expected_study_ids:\n",
        "                issues.append(f\"Study folder '{study}' not found in CSV\")\n",
        "\n",
        "            # Validate image files\n",
        "            for img in os.listdir(study_path):\n",
        "                if img.endswith(\".png\"):\n",
        "                    img_id = os.path.splitext(img)[0]  # remove extension\n",
        "                    if img_id not in expected_image_ids:\n",
        "                        issues.append(f\"Image '{img}' in '{study_path}' not found in CSV\")\n",
        "\n",
        "# Summary\n",
        "if not issues:\n",
        "    print(\"‚úÖ Dataset structure matches CSV metadata and is valid.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Issues found:\")\n",
        "    for issue in issues:\n",
        "        print(\"-\", issue)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF5zOxE0Fahz",
        "outputId": "0c05a661-9b2e-4d6b-9da3-23d231dafd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset structure matches CSV metadata and is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "csv_path = os.path.join(base_dir, \"image_df_preprocessed_fixed.csv\")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Iterate over all columns and print unique values and counts\n",
        "for col in df.columns:\n",
        "    unique_vals = df[col].nunique()\n",
        "    print(f\"{col}: {unique_vals} unique values\")\n",
        "    # Optionally, show top 10 most frequent values\n",
        "    print(df[col].value_counts().head(10))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emiQxS3a3OP8",
        "outputId": "da0a0ce4-17b4-4ed4-ed59-ddbeac012e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_id: 19996 unique values\n",
            "image_id\n",
            "7dbf6830cc06730cfe74cd58937f89a8    24\n",
            "6266ffa44d75d2edc9d3c725b20b6d49    24\n",
            "2bd9c72b886e97da1aff1361962c6acc    24\n",
            "2973bcf878fad1e9edade25be62602ce    24\n",
            "3cd51ee99070c4d625d52b848d5e9bfc    22\n",
            "10e0f362333df810ac84a9db8fb3fd42    22\n",
            "a7acc2e02a4944c4fc72e32507b17fa7    22\n",
            "85a6579cbdc403cfc4dde0a8149ed855    22\n",
            "136a7d195b654c4bf862fdd076c77574    21\n",
            "c0d6b03b2add28581aec656ad0d10613    21\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "study_id: 7999 unique values\n",
            "study_id\n",
            "77d2b897870fd48aacdc9b2bbad1ef52            4\n",
            "9b720facd58a23aef24bee0823ba5a27_dup2638    4\n",
            "28eb668601ac25349e36c8cdd040b41b            4\n",
            "7fa05485d8e90a042cd57d5bb2206b57            4\n",
            "7d62d74422a284f22062355f3c772c8f            4\n",
            "55a12c640ccebc100e67e21476b57285            4\n",
            "fe23c1647f7617ef219a0a0e07c9eec5_dup2724    4\n",
            "428b656fce3168763e8f2fccb4ffdfba_dup2685    4\n",
            "317986c9303c4a6b9e6d015d67baf4bf            4\n",
            "571e2132db7005421ab241f54439e7bb            4\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "filename: 19996 unique values\n",
            "filename\n",
            "7dbf6830cc06730cfe74cd58937f89a8.png    24\n",
            "6266ffa44d75d2edc9d3c725b20b6d49.png    24\n",
            "2bd9c72b886e97da1aff1361962c6acc.png    24\n",
            "2973bcf878fad1e9edade25be62602ce.png    24\n",
            "3cd51ee99070c4d625d52b848d5e9bfc.png    22\n",
            "10e0f362333df810ac84a9db8fb3fd42.png    22\n",
            "a7acc2e02a4944c4fc72e32507b17fa7.png    22\n",
            "85a6579cbdc403cfc4dde0a8149ed855.png    22\n",
            "136a7d195b654c4bf862fdd076c77574.png    21\n",
            "c0d6b03b2add28581aec656ad0d10613.png    21\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "birads: 5 unique values\n",
            "birads\n",
            "1    17098\n",
            "2     6620\n",
            "4     4922\n",
            "5     2226\n",
            "3     1130\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "birads_dir: 5 unique values\n",
            "birads_dir\n",
            "birads_1    17098\n",
            "birads_2     6620\n",
            "birads_4     4922\n",
            "birads_5     2226\n",
            "birads_3     1130\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "density: 4 unique values\n",
            "density\n",
            "C    24574\n",
            "B     3818\n",
            "D     3468\n",
            "A      136\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "laterality: 2 unique values\n",
            "laterality\n",
            "L    15998\n",
            "R    15998\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "view_position: 2 unique values\n",
            "view_position\n",
            "CC     15998\n",
            "MLO    15998\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "split: 2 unique values\n",
            "split\n",
            "training    27996\n",
            "test         4000\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_categories: 91 unique values\n",
            "finding_categories\n",
            "[\"['No Finding']\"]                                                           24116\n",
            "[\"['Mass']\"]                                                                  3128\n",
            "[\"['Suspicious Calcification']\"]                                               940\n",
            "[\"['Focal Asymmetry']\"]                                                        623\n",
            "[\"['Mass']\", \"['Suspicious Calcification']\"]                                   546\n",
            "[\"['Suspicious Calcification', 'Mass']\"]                                       366\n",
            "[\"['Architectural Distortion']\"]                                               366\n",
            "[\"['Suspicious Calcification', 'Focal Asymmetry']\"]                            132\n",
            "[\"['Asymmetry']\"]                                                              121\n",
            "[\"['Mass']\", \"['Suspicious Lymph Node']\", \"['Suspicious Calcification']\"]       99\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_birads_clean: 7 unique values\n",
            "finding_birads_clean\n",
            "[]        24146\n",
            "[4]        4537\n",
            "[5]        1296\n",
            "[3]        1015\n",
            "[5, 4]      809\n",
            "[4, 3]      191\n",
            "[5, 3]        2\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "xmin: 1760 unique values\n",
            "xmin\n",
            "1421.530029    24\n",
            "1438.170044    24\n",
            "2436.817041    22\n",
            "2465.381875    22\n",
            "346.046997     21\n",
            "143.108002     21\n",
            "2315.649902    19\n",
            "2078.209961    19\n",
            "1818.780029    18\n",
            "0.685536       18\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "ymin: 1759 unique values\n",
            "ymin\n",
            "577.869995     24\n",
            "1615.949951    24\n",
            "1833.121439    22\n",
            "485.898724     22\n",
            "1135.739990    21\n",
            "1000.580017    21\n",
            "1200.609985    19\n",
            "1161.699951    19\n",
            "1051.569946    19\n",
            "207.570007     18\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "xmax: 1760 unique values\n",
            "xmax\n",
            "2790.610107    29\n",
            "2008.739990    24\n",
            "1953.670044    24\n",
            "2672.780256    22\n",
            "2655.419922    22\n",
            "532.539002     21\n",
            "708.348999     21\n",
            "2671.090088    19\n",
            "554.994019     18\n",
            "662.369019     18\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "ymax: 1755 unique values\n",
            "ymax\n",
            "1998.660034    24\n",
            "1214.150024    24\n",
            "2142.227302    22\n",
            "2122.822289    22\n",
            "1581.560059    21\n",
            "1328.229980    21\n",
            "1401.140015    21\n",
            "1644.310059    20\n",
            "1857.670044    19\n",
            "1761.939941    19\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "has_bbox: 2 unique values\n",
            "has_bbox\n",
            "0    24116\n",
            "1     7880\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "age: 66 unique values\n",
            "age\n",
            "45    5678\n",
            "47    1244\n",
            "42    1244\n",
            "43    1148\n",
            "49    1100\n",
            "44    1068\n",
            "56    1042\n",
            "41    1016\n",
            "46     980\n",
            "51     980\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "birads_binary: 2 unique values\n",
            "birads_binary\n",
            "normal      23718\n",
            "abnormal     8278\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "birads_cleaned: 5 unique values\n",
            "birads_cleaned\n",
            "1    17098\n",
            "2     6620\n",
            "4     4922\n",
            "5     2226\n",
            "3     1130\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "birads_study_level: 5 unique values\n",
            "birads_study_level\n",
            "1    10060\n",
            "4     9472\n",
            "2     6268\n",
            "5     4452\n",
            "3     1744\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_mass: 2 unique values\n",
            "finding_mass\n",
            "0    26772\n",
            "1     5224\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_suspicious_calcification: 2 unique values\n",
            "finding_suspicious_calcification\n",
            "0    29016\n",
            "1     2980\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_focal_asymmetry: 2 unique values\n",
            "finding_focal_asymmetry\n",
            "0    30892\n",
            "1     1104\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_asymmetry: 2 unique values\n",
            "finding_asymmetry\n",
            "0    31801\n",
            "1      195\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_global_asymmetry: 2 unique values\n",
            "finding_global_asymmetry\n",
            "0    31930\n",
            "1       66\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_architectural_distortion: 2 unique values\n",
            "finding_architectural_distortion\n",
            "0    31248\n",
            "1      748\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_skin_thickening: 2 unique values\n",
            "finding_skin_thickening\n",
            "0    31672\n",
            "1      324\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_skin_retraction: 2 unique values\n",
            "finding_skin_retraction\n",
            "0    31851\n",
            "1      145\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_nipple_retraction: 2 unique values\n",
            "finding_nipple_retraction\n",
            "0    31739\n",
            "1      257\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_suspicious_lymph_node: 2 unique values\n",
            "finding_suspicious_lymph_node\n",
            "0    31580\n",
            "1      416\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "finding_no_finding: 2 unique values\n",
            "finding_no_finding\n",
            "1    24116\n",
            "0     7880\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "image_path: 31996 unique values\n",
            "image_path\n",
            "/content/birads_preprocessed_dataset/test/abnormal/77d2b897870fd48aacdc9b2bbad1ef52/cfd41653f3323981233f2f70146b0b60.png              1\n",
            "/content/birads_preprocessed_dataset/training/normal/9b720facd58a23aef24bee0823ba5a27_dup2638/0dc38f7b2936fff46ec10a0beb5aaeb2.png    1\n",
            "/content/birads_preprocessed_dataset/training/normal/9b720facd58a23aef24bee0823ba5a27_dup2638/338460bf0c6de576629169b75370964b.png    1\n",
            "/content/birads_preprocessed_dataset/training/normal/9b720facd58a23aef24bee0823ba5a27_dup2638/49e757c509c633f54b21447980a8354e.png    1\n",
            "/content/birads_preprocessed_dataset/training/normal/9b720facd58a23aef24bee0823ba5a27_dup2638/7eaef20a37390493cc62d4e272a5748b.png    1\n",
            "/content/birads_preprocessed_dataset/training/normal/28eb668601ac25349e36c8cdd040b41b/92dd8c4ba0e043d6201ff2aaa41d595a.png            1\n",
            "/content/birads_preprocessed_dataset/training/normal/28eb668601ac25349e36c8cdd040b41b/9e81152169d2561bc7ff19e9bd79aa78.png            1\n",
            "/content/birads_preprocessed_dataset/training/normal/28eb668601ac25349e36c8cdd040b41b/b1796339582de698af4fcbffe72642bd.png            1\n",
            "/content/birads_preprocessed_dataset/training/normal/28eb668601ac25349e36c8cdd040b41b/c33cabd280f1bf05f95cb1408452c1bb.png            1\n",
            "/content/birads_preprocessed_dataset/training/normal/7fa05485d8e90a042cd57d5bb2206b57/46d5e1b47ffcda5ecda23ec3364a4b5f.png            1\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "case_category: 3 unique values\n",
            "case_category\n",
            "0    18072\n",
            "1     9472\n",
            "2     4452\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "upsampled: 2 unique values\n",
            "upsampled\n",
            "False    19996\n",
            "True     12000\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n",
            "preprocessed_path: 31996 unique values\n",
            "preprocessed_path\n",
            "/content/birads_preprocessed_dataset/test/normal/e40b823436e86b4257917f258c24a87a/6ceb4652e62081e9013681238e62abcf.png                   1\n",
            "/content/birads_preprocessed_dataset/training/normal/fce53bbf985d83e76e3ce1403e8c8540/6986e869311e735b06accc480554e520.png               1\n",
            "/content/birads_preprocessed_dataset/training/normal/f1702c9e0f4c381fca530589c88356e2/649259e571b44ca2ba489d9c28fd2551.png               1\n",
            "/content/birads_preprocessed_dataset/training/normal/2cddf8995cd47a72e67b1cf6d10188f8/18c4e6b0bcf0420f6b72349db69cd872.png               1\n",
            "/content/birads_preprocessed_dataset/training/normal/7012aed1d6357e0291c62ed44902da58/7441ce1c9f8a9331b799faca56c3cff1.png               1\n",
            "/content/birads_preprocessed_dataset/training/abnormal/c3c0a901ca8edd8f87f4a0ecc3d2cca6_dup11211/d22bccb2cd637ba16a4a60db50cb2293.png    1\n",
            "/content/birads_preprocessed_dataset/training/normal/4ff54ff1105c5f62650c4692d67d2b97/e70431691ffb5b4aab0f98c1de3792ae.png               1\n",
            "/content/birads_preprocessed_dataset/training/abnormal/8abaf178e500859d0a532bf34a4ce595_dup1372/c07913233d9119af17fb07e09717aa30.png     1\n",
            "/content/birads_preprocessed_dataset/training/normal/779d6135933ba2e335846cdf10df650e/e05633f3efa21a54237901c14886fcf2.png               1\n",
            "/content/birads_preprocessed_dataset/training/normal/c488d7ef379be78343fd91eefa9cc477/16c5567c9a2c621c980a8992f6540c01.png               1\n",
            "Name: count, dtype: int64\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSQwO9gGGK5t",
        "outputId": "9b64ec06-6770-403b-ef58-6b9bb3327d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------------------------\n",
        "# Paths\n",
        "# ----------------------------\n",
        "base_dir = \"/content/birads_preprocessed_dataset\"\n",
        "csv_path = os.path.join(base_dir, \"image_df_preprocessed_fixed.csv\")\n",
        "\n",
        "# ----------------------------\n",
        "# Load CSV and map labels\n",
        "# ----------------------------\n",
        "df = pd.read_csv(csv_path)\n",
        "df[\"birads_binary\"] = df[\"birads_binary\"].map({\"normal\": 0, \"abnormal\": 1})  # map to numeric\n",
        "\n",
        "# ----------------------------\n",
        "# Image transforms\n",
        "# ----------------------------\n",
        "IMAGE_TRANSFORMS = {\n",
        "    \"train\": T.Compose([\n",
        "        T.Grayscale(num_output_channels=3),\n",
        "        T.Resize((224, 224)),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomRotation(10),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406],\n",
        "                    [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"val\": T.Compose([\n",
        "        T.Grayscale(num_output_channels=3),\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406],\n",
        "                    [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"test\": T.Compose([\n",
        "        T.Grayscale(num_output_channels=3),\n",
        "        T.Resize((224, 224)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406],\n",
        "                    [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Study-level Dataset\n",
        "# ----------------------------\n",
        "class MammogramStudyDataset(Dataset):\n",
        "    def __init__(self, df, split=\"training\", transform=None):\n",
        "        self.df_split = df[df[\"split\"] == split].copy()\n",
        "        self.transform = transform\n",
        "\n",
        "        # Group studies\n",
        "        self.study_groups = {}\n",
        "        for study_id, group in self.df_split.groupby(\"study_id\"):\n",
        "            valid_images = [row[\"image_path\"] for _, row in group.iterrows() if os.path.exists(row[\"image_path\"])]\n",
        "            if len(valid_images) == 4:  # keep only complete studies\n",
        "                self.study_groups[study_id] = {\n",
        "                    \"image_paths\": valid_images,\n",
        "                    \"label\": int(group[\"birads_binary\"].iloc[0])\n",
        "                }\n",
        "\n",
        "        self.study_ids = list(self.study_groups.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.study_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        study_id = self.study_ids[idx]\n",
        "        study_data = self.study_groups[study_id]\n",
        "\n",
        "        images = []\n",
        "        for img_path in study_data[\"image_paths\"]:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            images.append(img)\n",
        "\n",
        "        images = torch.stack(images)  # shape: (4, C, H, W)\n",
        "        label = study_data[\"label\"]\n",
        "\n",
        "        return images, label\n",
        "\n",
        "# ----------------------------\n",
        "# Create datasets\n",
        "# ----------------------------\n",
        "train_dataset = MammogramStudyDataset(df, split=\"training\", transform=IMAGE_TRANSFORMS[\"train\"])\n",
        "val_dataset   = MammogramStudyDataset(df, split=\"training\", transform=IMAGE_TRANSFORMS[\"val\"])\n",
        "test_dataset  = MammogramStudyDataset(df, split=\"test\", transform=IMAGE_TRANSFORMS[\"test\"])\n",
        "\n",
        "# Train/Val split by study\n",
        "train_ids, val_ids = train_test_split(train_dataset.study_ids, test_size=0.2, random_state=42)\n",
        "train_dataset.study_ids = train_ids\n",
        "val_dataset.study_ids = val_ids\n",
        "\n",
        "# ----------------------------\n",
        "# Dataloaders\n",
        "# ----------------------------\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Summary\n",
        "# ----------------------------\n",
        "print(\"Train studies:\", len(train_dataset))\n",
        "print(\"Val studies:\", len(val_dataset))\n",
        "print(\"Test studies:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IomEFbRbIaZ6",
        "outputId": "a5a3872f-eab4-44c9-f432-83624d60a5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train studies: 5599\n",
            "Val studies: 1400\n",
            "Test studies: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total studies in training split:\", len(train_dataset.study_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cGx7OJIwQsI",
        "outputId": "2129aa2e-5c4c-4adb-a422-e996937533c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total studies in training split: 5599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class StudyLevelResNet(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\", pretrained=True, num_classes=1):\n",
        "        super(StudyLevelResNet, self).__init__()\n",
        "\n",
        "        # Backbone selection\n",
        "        if backbone == \"resnet50\":\n",
        "            self.cnn = models.resnet50(pretrained=pretrained)\n",
        "            in_features = self.cnn.fc.in_features\n",
        "            self.cnn.fc = nn.Identity()  # remove final classification layer\n",
        "        elif backbone == \"efficientnet_b0\":\n",
        "            self.cnn = models.efficientnet_b0(pretrained=pretrained)\n",
        "            in_features = self.cnn.classifier[1].in_features\n",
        "            self.cnn.classifier = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # Study-level classification head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)  # binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, num_views, C, H, W)\n",
        "        num_views = 4 (study-level images)\n",
        "        \"\"\"\n",
        "        batch_size, num_views, C, H, W = x.shape\n",
        "        x = x.view(batch_size * num_views, C, H, W)  # flatten views into batch\n",
        "\n",
        "        # Extract features per view\n",
        "        feats = self.cnn(x)  # (batch_size*num_views, in_features)\n",
        "        feats = feats.view(batch_size, num_views, -1)  # group by study\n",
        "\n",
        "        # Fuse across views (mean pooling)\n",
        "        fused = feats.mean(dim=1)  # (batch_size, in_features)\n",
        "\n",
        "        # Classification head\n",
        "        out = self.fc(fused).squeeze(1)  # (batch_size,)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "cvggMtz09utI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Model setup\n",
        "model = StudyLevelResNet(backbone=\"resnet50\", pretrained=True, num_classes=1)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler (without verbose)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"max\",    # maximize monitored metric (e.g., val AUROC)\n",
        "    factor=0.5,    # reduce LR by 50%\n",
        "    patience=2     # wait 2 epochs before reducing\n",
        ")\n",
        "\n",
        "print(f\"Model ready on device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKqO8miuJUWd",
        "outputId": "673ccf1f-1833-4aa5-e801-b0ff5cf5a8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 169MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score, brier_score_loss\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics helper\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, y_pred_probs):\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"AUROC\": roc_auc_score(y_true, y_pred_probs),\n",
        "        \"AUPRC\": average_precision_score(y_true, y_pred_probs),\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"Brier\": brier_score_loss(y_true, y_pred_probs)\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Training one epoch\n",
        "# -----------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "    return total_loss / len(loader.dataset), metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Validation\n",
        "# -----------------------------\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    loop = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.float().to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "    return total_loss / len(loader.dataset), metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Full training loop\n",
        "# -----------------------------\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=10):\n",
        "    best_val_auroc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
        "\n",
        "        train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Train AUROC: {train_metrics['AUROC']:.4f} | Val AUROC: {val_metrics['AUROC']:.4f}\")\n",
        "        print(f\"Train Accuracy: {train_metrics['Accuracy']:.4f} | Val Accuracy: {val_metrics['Accuracy']:.4f}\")\n",
        "\n",
        "        # Step scheduler with validation AUROC\n",
        "        scheduler.step(val_metrics['AUROC'])\n",
        "\n",
        "        # Optional: save best model\n",
        "        if val_metrics['AUROC'] > best_val_auroc:\n",
        "            best_val_auroc = val_metrics['AUROC']\n",
        "            torch.save(model.state_dict(), \"best_studylevel_model.pth\")\n",
        "            print(\"‚úÖ Saved new best model.\")\n",
        "\n",
        "    print(f\"\\nTraining complete. Best Val AUROC: {best_val_auroc:.4f}\")\n"
      ],
      "metadata": {
        "id": "8cVT3E4gJgtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Test evaluation\n",
        "# -----------------------------\n",
        "def evaluate_test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    loop = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.float().to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    metrics = compute_metrics(all_labels, all_preds)\n",
        "\n",
        "    print(\"\\n=== Test Metrics ===\")\n",
        "    print(f\"AUROC  : {metrics['AUROC']:.4f}\")\n",
        "    print(f\"AUPRC  : {metrics['AUPRC']:.4f}\")\n",
        "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "    print(f\"F1     : {metrics['F1']:.4f}\")\n",
        "    print(f\"Brier  : {metrics['Brier']:.4f}\")\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "I9qtaoxLJiwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def run_training(model, train_loader, val_loader, test_loader, device,\n",
        "                 epochs=10, lr=1e-4, weight_decay=1e-4, checkpoint_path=\"best_model.pth\"):\n",
        "\n",
        "    # Loss, optimizer, scheduler\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "\n",
        "    best_val_auroc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # -----------------------------\n",
        "        # Training\n",
        "        # -----------------------------\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        all_labels, all_preds = [], []\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\", leave=False)\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "\n",
        "        # -----------------------------\n",
        "        # Validation\n",
        "        # -----------------------------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_labels, all_preds = [], []\n",
        "\n",
        "        loop = tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for images, labels in loop:\n",
        "                images, labels = images.to(device), labels.float().to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "\n",
        "                loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Val AUROC: {val_metrics['AUROC']:.4f}\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Scheduler & Checkpoint\n",
        "        # -----------------------------\n",
        "        scheduler.step(val_metrics['AUROC'])\n",
        "\n",
        "        if val_metrics['AUROC'] > best_val_auroc:\n",
        "            best_val_auroc = val_metrics['AUROC']\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"‚úÖ Saved best model at epoch {epoch} (Val AUROC: {best_val_auroc:.4f})\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load best model & test\n",
        "    # -----------------------------\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.to(device)\n",
        "    print(\"\\n=== Evaluating on Test Set ===\")\n",
        "    test_metrics = evaluate_test(model, test_loader, device)\n",
        "\n",
        "    return model, train_metrics, val_metrics, test_metrics\n"
      ],
      "metadata": {
        "id": "E5kxoGaX59jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StudyLevelResNet(backbone=\"resnet50\", pretrained=True).to(device)\n",
        "'''\n",
        "trained_model, train_metrics, val_metrics, test_metrics = run_training(\n",
        "    model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    epochs=10,\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    checkpoint_path=\"best_studylevel_model.pth\"\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "fkSlo_j76L72",
        "outputId": "0002f2d4-35f9-4515-8d47-c9a2bf41d466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrained_model, train_metrics, val_metrics, test_metrics = run_training(\\n    model,\\n    train_loader=train_loader,\\n    val_loader=val_loader,\\n    test_loader=test_loader,\\n    device=device,\\n    epochs=10,\\n    lr=1e-4,\\n    weight_decay=1e-4,\\n    checkpoint_path=\"best_studylevel_model.pth\"\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class StudyLevelResNetAttention(nn.Module):\n",
        "    def __init__(self, backbone=\"resnet50\", pretrained=True, num_classes=1, attention_dim=128):\n",
        "        super(StudyLevelResNetAttention, self).__init__()\n",
        "\n",
        "        # Backbone\n",
        "        if backbone == \"resnet50\":\n",
        "            self.cnn = models.resnet50(pretrained=pretrained)\n",
        "            in_features = self.cnn.fc.in_features\n",
        "            self.cnn.fc = nn.Identity()\n",
        "        elif backbone == \"efficientnet_b0\":\n",
        "            self.cnn = models.efficientnet_b0(pretrained=pretrained)\n",
        "            in_features = self.cnn.classifier[1].in_features\n",
        "            self.cnn.classifier = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # Attention layers\n",
        "        self.attention_fc = nn.Sequential(\n",
        "            nn.Linear(in_features, attention_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(attention_dim, 1)  # scalar attention score per view\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)  # binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, num_views, C, H, W)\n",
        "        \"\"\"\n",
        "        batch_size, num_views, C, H, W = x.shape\n",
        "        x = x.view(batch_size * num_views, C, H, W)\n",
        "\n",
        "        # Extract features for each view\n",
        "        feats = self.cnn(x)  # (batch_size*num_views, in_features)\n",
        "        feats = feats.view(batch_size, num_views, -1)  # (batch_size, num_views, in_features)\n",
        "\n",
        "        # Attention scores\n",
        "        attn_scores = self.attention_fc(feats)  # (batch_size, num_views, 1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)  # normalize over views\n",
        "\n",
        "        # Weighted sum of features\n",
        "        fused = (feats * attn_weights).sum(dim=1)  # (batch_size, in_features)\n",
        "\n",
        "        # Classification\n",
        "        out = self.fc(fused).squeeze(1)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "czl9zOixJEfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score, brier_score_loss\n",
        "\n",
        "# -----------------------------\n",
        "# Device & seed\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Metrics\n",
        "# -----------------------------\n",
        "def compute_metrics(y_true, y_pred_probs):\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"AUROC\": roc_auc_score(y_true, y_pred_probs),\n",
        "        \"AUPRC\": average_precision_score(y_true, y_pred_probs),\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"F1\": f1_score(y_true, y_pred),\n",
        "        \"Brier\": brier_score_loss(y_true, y_pred_probs)\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Train for one epoch\n",
        "# -----------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "    return total_loss / len(loader.dataset), metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Validation / Test\n",
        "# -----------------------------\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_labels, all_preds = [], []\n",
        "\n",
        "    loop = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loop:\n",
        "            images, labels = images.to(device), labels.float().to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
        "    return total_loss / len(loader.dataset), metrics\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize model, loss, optimizer\n",
        "# -----------------------------\n",
        "model = StudyLevelResNetAttention(backbone=\"resnet50\", pretrained=True).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "num_epochs = 10\n",
        "best_val_auroc = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss, train_metrics = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Adjust LR\n",
        "    scheduler.step(val_metrics[\"AUROC\"])\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, AUROC: {train_metrics['AUROC']:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, AUROC: {val_metrics['AUROC']:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_metrics[\"AUROC\"] > best_val_auroc:\n",
        "        best_val_auroc = val_metrics[\"AUROC\"]\n",
        "        torch.save(model.state_dict(), \"best_study_attention_model.pth\")\n",
        "        print(\"‚úÖ Saved best model\")\n"
      ],
      "metadata": {
        "id": "0uTkYl2g6P2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69dcd76-5af3-4ce6-e4fe-a9584504232b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5943, AUROC: 0.5644\n",
            "Val Loss: 0.7483, AUROC: 0.4898\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5899, AUROC: 0.5725\n",
            "Val Loss: 0.5901, AUROC: 0.6254\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5893, AUROC: 0.5781\n",
            "Val Loss: 0.5774, AUROC: 0.6521\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5810, AUROC: 0.6095\n",
            "Val Loss: 0.5649, AUROC: 0.6763\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5672, AUROC: 0.6539\n",
            "Val Loss: 0.5558, AUROC: 0.6987\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5580, AUROC: 0.6714\n",
            "Val Loss: 0.5632, AUROC: 0.6831\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5440, AUROC: 0.7031\n",
            "Val Loss: 0.5496, AUROC: 0.7272\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5287, AUROC: 0.7287\n",
            "Val Loss: 0.5339, AUROC: 0.7295\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5185, AUROC: 0.7428\n",
            "Val Loss: 0.4757, AUROC: 0.8201\n",
            "‚úÖ Saved best model\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4857, AUROC: 0.7875\n",
            "Val Loss: 0.4572, AUROC: 0.8367\n",
            "‚úÖ Saved best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Load best model\n",
        "# -----------------------------\n",
        "best_model = StudyLevelResNetAttention(backbone=\"resnet50\", pretrained=False).to(device)\n",
        "best_model.load_state_dict(torch.load(\"best_study_attention_model.pth\"))\n",
        "\n",
        "# -----------------------------\n",
        "# Run on test set\n",
        "# -----------------------------\n",
        "test_metrics = evaluate_test(best_model, test_loader, device)\n",
        "\n",
        "print(\"\\n=== Final Test Results ===\")\n",
        "for k, v in test_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9u9NVeJCCiD",
        "outputId": "5ffd685b-a9a0-41a6-c7c8-5dff9cb34005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Metrics ===\n",
            "AUROC  : 0.6272\n",
            "AUPRC  : 0.1357\n",
            "Accuracy: 0.8870\n",
            "F1     : 0.0342\n",
            "Brier  : 0.0942\n",
            "\n",
            "=== Final Test Results ===\n",
            "AUROC: 0.6272\n",
            "AUPRC: 0.1357\n",
            "Accuracy: 0.8870\n",
            "F1: 0.0342\n",
            "Brier: 0.0942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zs_UnCW6uAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}